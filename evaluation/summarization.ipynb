{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice using an LLM as an Evaluator/Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import AzureOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from datasets import load_dataset\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import json\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from azure.identity import (\n",
    "    DefaultAzureCredential,\n",
    ")\n",
    "\n",
    "from azure.identity import AzureAuthorityHosts\n",
    "from azure.keyvault.secrets import SecretClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your Azure OpenAI Resource and Key Vault\n",
    "\n",
    "_If you have your resource from the last exercise, you don't need to complete the following steps._\n",
    "\n",
    "Navigate to the [Azure Portal](https://portal.azure.com/#home) or [US Gov Azure Portal](https://portal.azure.us/#home) and login using your account. Next you're going to create an Azure OpenAI resource, create a new resource group and use any unique name for the resource's name.  \n",
    "Once the resource is created, you need to open [Azure AI Foundry](https://ai.azure.com/) or [Azure OpenAI Studio](https://ai.azure.us/) to deploy the model. Navigate to deployments, press deploy model and select gpt-4o-mini.  Make sure to increase your rate limit, or tokens per minute (around 700k should be sufficient)\n",
    "\n",
    "Once that is created, copy the key (Found under Resource Management > Keys and Endpoints) and create a new key vault. Assign the same resource group as your Azure OpenAI resource and again pick a unique name for the key vault name.  \n",
    "\n",
    "Once the key vault is created, make a new secret with the API key.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Azure OpenAI\n",
    "First, run `az login` in the terminal and login to your FedAIRS account.  \n",
    "<br>\n",
    "If you are using a Gov account:<br>\n",
    "az cloud set --name AzureUSGovernment <br>\n",
    "az login <br>\n",
    "az account set --subscription=\"your subscription\"<br>\n",
    "\n",
    "If you are using a commercial account:<br>\n",
    "az login <br>\n",
    "az account set --subscription=\"your subscription\"<br>\n",
    "<br>\n",
    "Two things are needed to connect to your Azure OpenAI resource\n",
    "- Your API key\n",
    "- Your Endpoint  \n",
    "  \n",
    "For the API Key, we are going to connect to the key vault we just made to insert the key. For this, you'll need to change the URL below to match your key vault's URL.  \n",
    "Next, we will insert the endpoint URL from our Azure OpenAI resource.\n",
    "\n",
    "Our `azure_client` is where we are calling the LLM and connecting to the model we deployed. Other parameters can be passed in, like timeout or max_retires.\n",
    "\n",
    "Note: In the block below we inserted `credential = DefaultAzureCredential(authority=AzureAuthorityHosts.AZURE_GOVERNMENT)`. This is because when we use our FedAIRS account we are connected to the US Government cloud, rather than the regular commercial cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "credential = DefaultAzureCredential(authority=AzureAuthorityHosts.AZURE_GOVERNMENT)\n",
    "\n",
    "secret_client = SecretClient(vault_url=os.getenv('KEY_VAULT_URL'), credential=credential)\n",
    "deployment = os.getenv('DEPLOYMENT')\n",
    "endpoint_url = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "api_version = os.getenv('API_VERSION')\n",
    "api_key = secret_client.get_secret(os.getenv('SECRET_NAME')).value\n",
    "\n",
    "\n",
    "azure_client = AzureChatOpenAI(\n",
    "                api_key=api_key\n",
    "                ,api_version=api_version\n",
    "                ,azure_endpoint=endpoint_url\n",
    "                ,deployment_name=deployment\n",
    "                ,temperature=0\n",
    "                ,max_tokens=4000\n",
    "                ,model_kwargs={\"response_format\": {\"type\": \"json_object\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Dataset\n",
    "\n",
    "Like the last exercise, we are going to pull data from [Hugging Face](https://huggingface.co/). The [SAMsum dataset](https://huggingface.co/datasets/Samsung/samsum) contains about 16k messenger-like conversations with summaries. Conversations were created and written down by linguists fluent in English. Linguists were asked to create conversations similar to those they write on a daily basis, reflecting the proportion of topics of their real-life messenger convesations. The style and register are diversified - conversations could be informal, semi-formal or formal, they may contain slang words, emoticons and typos. Then, the conversations were annotated with summaries. It was assumed that summaries should be a concise brief of what people talked about in the conversation in third person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (C:/Users/mislentz/.cache/huggingface/datasets/Samsung___samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13818513</td>\n",
       "      <td>Amanda: I baked  cookies. Do you want some?\\r\\...</td>\n",
       "      <td>Amanda baked cookies and will bring Jerry some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13728867</td>\n",
       "      <td>Olivia: Who are you voting for in this electio...</td>\n",
       "      <td>Olivia and Olivier are voting for liberals in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13681000</td>\n",
       "      <td>Tim: Hi, what's up?\\r\\nKim: Bad mood tbh, I wa...</td>\n",
       "      <td>Kim may try the pomodoro technique recommended...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13730747</td>\n",
       "      <td>Edward: Rachel, I think I'm in ove with Bella....</td>\n",
       "      <td>Edward thinks he is in love with Bella. Rachel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13728094</td>\n",
       "      <td>Sam: hey  overheard rick say something\\r\\nSam:...</td>\n",
       "      <td>Sam is confused, because he overheard Rick com...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           dialogue  \\\n",
       "0  13818513  Amanda: I baked  cookies. Do you want some?\\r\\...   \n",
       "1  13728867  Olivia: Who are you voting for in this electio...   \n",
       "2  13681000  Tim: Hi, what's up?\\r\\nKim: Bad mood tbh, I wa...   \n",
       "3  13730747  Edward: Rachel, I think I'm in ove with Bella....   \n",
       "4  13728094  Sam: hey  overheard rick say something\\r\\nSam:...   \n",
       "\n",
       "                                             summary  \n",
       "0  Amanda baked cookies and will bring Jerry some...  \n",
       "1  Olivia and Olivier are voting for liberals in ...  \n",
       "2  Kim may try the pomodoro technique recommended...  \n",
       "3  Edward thinks he is in love with Bella. Rachel...  \n",
       "4  Sam is confused, because he overheard Rick com...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_dataset=load_dataset(\"Samsung/samsum\", split=\"train\")\n",
    "summaries = summary_dataset.to_pandas()\n",
    "summaries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like last time, we are going to take a sample of 100 rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_sample = summaries.sample(100).reset_index()\n",
    "summaries_sample_array = summaries_sample.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the summaries\n",
    "\n",
    "From our sample dataset, we are going to use the LLM to evaluate the summaries that the SAMsum dataset provided. We will use a few criteria:  \n",
    "* **Coherence:** the collective quality of all sentences. We align this dimension with the DUC quality question of structure and coherence whereby \"the summary should be well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to a coherent body of information about a topic\". Scored from 1-5.\n",
    "* **Consistency:** the factual alignment between the summary and the summarized source. A factually consistent summary contains only statements that are entailed by the source document. Annotators were also asked to penalize summaries that contained hallucinated facts. Scored from 1-5.\n",
    "* **Fluency:** the quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure. Scored from 1-3.\n",
    "* **Relevance:** selection of important content from the source. The summary should include only important information from the source document. Annotators were instructed to penalize summaries which contained redundancies and excess information. Scored from 1-5.\n",
    "\n",
    "We are prompting the LLM to give a score for each of the summaries and provide reasoning as to why that score was chosen. Then we will join all of the results together into one dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_prompt = PromptTemplate.from_template(\n",
    "\n",
    "    template = '''\n",
    "### Instructions\n",
    "You will be given one summary written for a source document.\n",
    "Your task is to rate the summary on one metric.\n",
    "Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "### Evaluation Criteria:\n",
    "Coherence (1-5) - the collective quality of all sentences. We align this dimension with the DUC quality question of structure and coherence whereby \"the summary should be well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to a coherent body of information about a topic.\"\n",
    "\n",
    "### Evaluation Steps:\n",
    "1. Read the source document carefully and identify the main topic and key points.\n",
    "2. Read the summary and compare it to the source document. Check if the summary covers the main topic and key points of the source document, and if it presents them in a clear and logical order.\n",
    "3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.  \n",
    "4. Return your score in JSON\n",
    "\n",
    "### Source Document:\n",
    "{Document}\n",
    "\n",
    "### Summary:\n",
    "{Summary}\n",
    "\n",
    "### Return JSON:\n",
    "{{\n",
    "    \"coherence\": <coherence score from 1-5>,\n",
    "    \"coherence_rationale\": \"<Explain why you assigned the score>\"\n",
    "}}\n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_chain = (\n",
    "    coherence_prompt\n",
    "    | azure_client\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "consistency_prompt = PromptTemplate.from_template(\n",
    "\n",
    "    template = '''\n",
    "### Instructions:\n",
    "You will be given a source document. You will then be given one summary written for this source document.\n",
    "Your task is to rate the summary on one metric.\n",
    "Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "### Evaluation Criteria:\n",
    "Consistency (1-5) - the factual alignment between the summary and the summarized source. A factually consistent summary contains only statements that are entailed by the source document. Annotators were also asked to penalize summaries that contained hallucinated facts. \n",
    "\n",
    "### Evaluation Steps:\n",
    "1. Read the source document carefully and identify the main facts and details it presents.\n",
    "2. Read the summary and compare it to the source document. Check if the summary contains any factual errors that are not supported by the source document.\n",
    "3. Assign a score for consistency based on the Evaluation Criteria.\n",
    "4. Return your score in JSON\n",
    "\n",
    "### Source Document: \n",
    "{Document}\n",
    "\n",
    "### Summary: \n",
    "{Summary}\n",
    "\n",
    "### Return JSON:\n",
    "{{\n",
    "    \"consistency\": <consistency score from 1-5>,\n",
    "    \"consistency_rationale\": \"<Explain why you assigned the score>\"\n",
    "}}\n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "consistency_chain = (\n",
    "    consistency_prompt\n",
    "    | azure_client\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluency_prompt = PromptTemplate.from_template(\n",
    "\n",
    "    template = '''\n",
    "### Instructions:\n",
    "You will be given one summary written for a source document.\n",
    "Your task is to rate the summary on one metric.\n",
    "Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\n",
    "Return your score in JSON\n",
    "\n",
    "### Evaluation Criteria:\n",
    "Fluency (1-3): the quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure.\n",
    "- 1: Poor. The summary has many errors that make it hard to understand or sound unnatural.\n",
    "- 2: Fair. The summary has some errors that affect the clarity or smoothness of the text, but the main points are still comprehensible.\n",
    "- 3: Good. The summary has few or no errors and is easy to read and follow.\n",
    "\n",
    "### Summary:\n",
    "{{Summary}}\n",
    "\n",
    "### Return JSON:\n",
    "{{\n",
    "    \"fluency\": <fluency score from 1-3>,\n",
    "    \"fluency_rationale\": \"<Explain why you assigned the score>\"\n",
    "}}\n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluency_chain = (\n",
    "    fluency_prompt\n",
    "    | azure_client\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_prompt = PromptTemplate.from_template(\n",
    "\n",
    "    template = '''\n",
    "### Instructions:\n",
    "You will be given one summary written for a source document.\n",
    "Your task is to rate the summary on one metric.\n",
    "Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "\n",
    "### Evaluation Criteria:\n",
    "Relevance (1-5) - selection of important content from the source. The summary should include only important information from the source document. Annotators were instructed to penalize summaries which contained redundancies and excess information.\n",
    "\n",
    "### Evaluation Steps:\n",
    "1. Read the summary and the source document carefully.\n",
    "2. Compare the summary to the source document and identify the main points of the source document.\n",
    "3. Assess how well the summary covers the main points of the source document, and how much irrelevant or redundant information it contains.\n",
    "4. Assign a relevance score from 1 to 5.\n",
    "\n",
    "### Source Document:\n",
    "{Document}\n",
    "\n",
    "### Summary:\n",
    "{Summary}\n",
    "\n",
    "\n",
    "### Return JSON:\n",
    "{{\n",
    "    \"relevance\": <relevance score from 1-5>,\n",
    "    \"relevance_rationale\": \"<Explain why you assigned the score>\"\n",
    "}}\n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_chain = (\n",
    "    relevance_prompt\n",
    "    | azure_client\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def runChain(summary_array, chain):\n",
    "    texts = [{\"Document\": x[2], \"Summary\": x[3]} for x in summary_array]\n",
    "    return await chain.abatch(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def assess_summaries(df, array, chain):\n",
    "    results = await runChain(array, chain)\n",
    "    df[\"results\"] = results\n",
    "\n",
    "    results_df = pd.json_normalize(df.results.apply(json.loads))\n",
    "    output = pd.concat([df, results_df], axis=1)\n",
    "    output.drop(columns='results', inplace=True)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to join all of the responses together to one dataframe for evaluation. Each row is one summary with its scores for coherence, consistency, fluency, and relevance as well as the reasoning for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=PromptTemplate(input_variables=['Document', 'Summary'], template='\\n### Instructions\\nYou will be given one summary written for a source document.\\nYour task is to rate the summary on one metric.\\nPlease make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\\n\\n### Evaluation Criteria:\\nCoherence (1-5) - the collective quality of all sentences. We align this dimension with the DUC quality question of structure and coherence whereby \"the summary should be well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to a coherent body of information about a topic.\"\\n\\n### Evaluation Steps:\\n1. Read the source document carefully and identify the main topic and key points.\\n2. Read the summary and compare it to the source document. Check if the summary covers the main topic and key points of the source document, and if it presents them in a clear and logical order.\\n3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.  \\n4. Return your score in JSON\\n\\n### Source Document:\\n{Document}\\n\\n### Summary:\\n{Summary}\\n\\n### Return JSON:\\n{{\\n    \"coherence\": <coherence score from 1-5>,\\n    \"coherence_rationale\": \"<Explain why you assigned the score>\"\\n}}\\n') middle=[AzureChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000245DAD5A7D0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000245D668D290>, temperature=0.0, model_kwargs={'response_format': {'type': 'json_object'}}, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=4000, azure_endpoint='https://mjs-exemplar-aoai-az.openai.azure.us/', deployment_name='gpt-4o-mini', openai_api_version='2024-10-21', openai_api_type='azure')] last=StrOutputParser()\n",
      "first=PromptTemplate(input_variables=['Document', 'Summary'], template='\\n### Instructions:\\nYou will be given a source document. You will then be given one summary written for this source document.\\nYour task is to rate the summary on one metric.\\nPlease make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\\n\\n### Evaluation Criteria:\\nConsistency (1-5) - the factual alignment between the summary and the summarized source. A factually consistent summary contains only statements that are entailed by the source document. Annotators were also asked to penalize summaries that contained hallucinated facts. \\n\\n### Evaluation Steps:\\n1. Read the source document carefully and identify the main facts and details it presents.\\n2. Read the summary and compare it to the source document. Check if the summary contains any factual errors that are not supported by the source document.\\n3. Assign a score for consistency based on the Evaluation Criteria.\\n4. Return your score in JSON\\n\\n### Source Document: \\n{Document}\\n\\n### Summary: \\n{Summary}\\n\\n### Return JSON:\\n{{\\n    \"consistency\": <consistency score from 1-5>,\\n    \"consistency_rationale\": \"<Explain why you assigned the score>\"\\n}}\\n') middle=[AzureChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000245DAD5A7D0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000245D668D290>, temperature=0.0, model_kwargs={'response_format': {'type': 'json_object'}}, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=4000, azure_endpoint='https://mjs-exemplar-aoai-az.openai.azure.us/', deployment_name='gpt-4o-mini', openai_api_version='2024-10-21', openai_api_type='azure')] last=StrOutputParser()\n",
      "first=PromptTemplate(input_variables=[], template='\\n### Instructions:\\nYou will be given one summary written for a source document.\\nYour task is to rate the summary on one metric.\\nPlease make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\\nReturn your score in JSON\\n\\n### Evaluation Criteria:\\nFluency (1-3): the quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure.\\n- 1: Poor. The summary has many errors that make it hard to understand or sound unnatural.\\n- 2: Fair. The summary has some errors that affect the clarity or smoothness of the text, but the main points are still comprehensible.\\n- 3: Good. The summary has few or no errors and is easy to read and follow.\\n\\n### Summary:\\n{{Summary}}\\n\\n### Return JSON:\\n{{\\n    \"fluency\": <fluency score from 1-3>,\\n    \"fluency_rationale\": \"<Explain why you assigned the score>\"\\n}}\\n') middle=[AzureChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000245DAD5A7D0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000245D668D290>, temperature=0.0, model_kwargs={'response_format': {'type': 'json_object'}}, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=4000, azure_endpoint='https://mjs-exemplar-aoai-az.openai.azure.us/', deployment_name='gpt-4o-mini', openai_api_version='2024-10-21', openai_api_type='azure')] last=StrOutputParser()\n",
      "first=PromptTemplate(input_variables=['Document', 'Summary'], template='\\n### Instructions:\\nYou will be given one summary written for a source document.\\nYour task is to rate the summary on one metric.\\nPlease make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\\n\\n\\n### Evaluation Criteria:\\nRelevance (1-5) - selection of important content from the source. The summary should include only important information from the source document. Annotators were instructed to penalize summaries which contained redundancies and excess information.\\n\\n### Evaluation Steps:\\n1. Read the summary and the source document carefully.\\n2. Compare the summary to the source document and identify the main points of the source document.\\n3. Assess how well the summary covers the main points of the source document, and how much irrelevant or redundant information it contains.\\n4. Assign a relevance score from 1 to 5.\\n\\n### Source Document:\\n{Document}\\n\\n### Summary:\\n{Summary}\\n\\n\\n### Return JSON:\\n{{\\n    \"relevance\": <fluency score from 1-5>,\\n    \"relevance_rationale\": \"<Explain why you assigned the score>\"\\n}}\\n') middle=[AzureChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000245DAD5A7D0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000245D668D290>, temperature=0.0, model_kwargs={'response_format': {'type': 'json_object'}}, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=4000, azure_endpoint='https://mjs-exemplar-aoai-az.openai.azure.us/', deployment_name='gpt-4o-mini', openai_api_version='2024-10-21', openai_api_type='azure')] last=StrOutputParser()\n"
     ]
    }
   ],
   "source": [
    "input_df = summaries_sample\n",
    "input_array = summaries_sample_array\n",
    "chains = [coherence_chain, consistency_chain, fluency_chain, relevance_chain]\n",
    "output_df = None\n",
    "\n",
    "for chain in chains:\n",
    "    print(chain)\n",
    "    output_df = await assess_summaries(input_df, input_array, chain)\n",
    "    input_df = output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>summary</th>\n",
       "      <th>coherence</th>\n",
       "      <th>coherence_rationale</th>\n",
       "      <th>consistency</th>\n",
       "      <th>consistency_rationale</th>\n",
       "      <th>fluency</th>\n",
       "      <th>fluency_rationale</th>\n",
       "      <th>relevance</th>\n",
       "      <th>relevance_rationale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5285</td>\n",
       "      <td>13611619</td>\n",
       "      <td>Kitty: Are you there?\\r\\nAnne: sure, what's up...</td>\n",
       "      <td>Kitty forgot the keys and can't get inside. An...</td>\n",
       "      <td>4</td>\n",
       "      <td>The summary effectively captures the main even...</td>\n",
       "      <td>5</td>\n",
       "      <td>The summary accurately reflects the main facts...</td>\n",
       "      <td>3</td>\n",
       "      <td>The summary is well-written with correct gramm...</td>\n",
       "      <td>5</td>\n",
       "      <td>The summary effectively captures the main poin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12541</td>\n",
       "      <td>13820714</td>\n",
       "      <td>Henryk: Have you seen the new episode? ;)\\r\\nP...</td>\n",
       "      <td>Henryk and Piotr want to discuss the latest ep...</td>\n",
       "      <td>5</td>\n",
       "      <td>The summary effectively captures the main topi...</td>\n",
       "      <td>5</td>\n",
       "      <td>The summary accurately reflects the main facts...</td>\n",
       "      <td>3</td>\n",
       "      <td>The summary is well-written with correct gramm...</td>\n",
       "      <td>5</td>\n",
       "      <td>The summary accurately captures the main point...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12420</td>\n",
       "      <td>13730466</td>\n",
       "      <td>Robert: yo did u talk to the teacher\\r\\nBale: ...</td>\n",
       "      <td>Robert and Bale have to submit the project. Ba...</td>\n",
       "      <td>4</td>\n",
       "      <td>The summary captures the main topic of Robert ...</td>\n",
       "      <td>4</td>\n",
       "      <td>The summary accurately captures the main point...</td>\n",
       "      <td>3</td>\n",
       "      <td>The summary is well-written with correct gramm...</td>\n",
       "      <td>4</td>\n",
       "      <td>The summary captures the main points of the co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1131</td>\n",
       "      <td>13729470</td>\n",
       "      <td>Tom: &lt;file_other&gt; read it!\\r\\nBridget: gosh, h...</td>\n",
       "      <td>Tom and Brigdet discuss an event involving an ...</td>\n",
       "      <td>4</td>\n",
       "      <td>The summary presents the main topic of the con...</td>\n",
       "      <td>5</td>\n",
       "      <td>The summary accurately reflects the main point...</td>\n",
       "      <td>3</td>\n",
       "      <td>The summary is well-written with correct gramm...</td>\n",
       "      <td>4</td>\n",
       "      <td>The summary captures the main points of the co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8830</td>\n",
       "      <td>13810253</td>\n",
       "      <td>Ross: Hey\\r\\nRoss: I am at the mall\\r\\nRoss: W...</td>\n",
       "      <td>Ross is at the mall and Rose prefers pure blac...</td>\n",
       "      <td>4</td>\n",
       "      <td>The summary effectively captures the main topi...</td>\n",
       "      <td>5</td>\n",
       "      <td>The summary accurately reflects the main facts...</td>\n",
       "      <td>3</td>\n",
       "      <td>The summary is well-written with correct gramm...</td>\n",
       "      <td>5</td>\n",
       "      <td>The summary accurately captures the main point...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index        id                                           dialogue  \\\n",
       "0   5285  13611619  Kitty: Are you there?\\r\\nAnne: sure, what's up...   \n",
       "1  12541  13820714  Henryk: Have you seen the new episode? ;)\\r\\nP...   \n",
       "2  12420  13730466  Robert: yo did u talk to the teacher\\r\\nBale: ...   \n",
       "3   1131  13729470  Tom: <file_other> read it!\\r\\nBridget: gosh, h...   \n",
       "4   8830  13810253  Ross: Hey\\r\\nRoss: I am at the mall\\r\\nRoss: W...   \n",
       "\n",
       "                                             summary  coherence  \\\n",
       "0  Kitty forgot the keys and can't get inside. An...          4   \n",
       "1  Henryk and Piotr want to discuss the latest ep...          5   \n",
       "2  Robert and Bale have to submit the project. Ba...          4   \n",
       "3  Tom and Brigdet discuss an event involving an ...          4   \n",
       "4  Ross is at the mall and Rose prefers pure blac...          4   \n",
       "\n",
       "                                 coherence_rationale  consistency  \\\n",
       "0  The summary effectively captures the main even...            5   \n",
       "1  The summary effectively captures the main topi...            5   \n",
       "2  The summary captures the main topic of Robert ...            4   \n",
       "3  The summary presents the main topic of the con...            5   \n",
       "4  The summary effectively captures the main topi...            5   \n",
       "\n",
       "                               consistency_rationale  fluency  \\\n",
       "0  The summary accurately reflects the main facts...        3   \n",
       "1  The summary accurately reflects the main facts...        3   \n",
       "2  The summary accurately captures the main point...        3   \n",
       "3  The summary accurately reflects the main point...        3   \n",
       "4  The summary accurately reflects the main facts...        3   \n",
       "\n",
       "                                   fluency_rationale  relevance  \\\n",
       "0  The summary is well-written with correct gramm...          5   \n",
       "1  The summary is well-written with correct gramm...          5   \n",
       "2  The summary is well-written with correct gramm...          4   \n",
       "3  The summary is well-written with correct gramm...          4   \n",
       "4  The summary is well-written with correct gramm...          5   \n",
       "\n",
       "                                 relevance_rationale  \n",
       "0  The summary effectively captures the main poin...  \n",
       "1  The summary accurately captures the main point...  \n",
       "2  The summary captures the main points of the co...  \n",
       "3  The summary captures the main points of the co...  \n",
       "4  The summary accurately captures the main point...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of one of the coherence evaluations. Below is the short dialogue as well as the score and the reasoning for why it was given that score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example Record:\n",
      "Dialogue: \tKitty: Are you there?\n",
      "Anne: sure, what's up?\n",
      "Kitty: I forgot the keys:/ again...\n",
      "Anne: Nooo... I am totally across town, can't come now\n",
      "Kitty: Shit;/ I gotta get ready for a meeting\n",
      "Anne: How about if I uber them to you?\n",
      "Kitty: Could you? That would be great! Where are you I'll order it\n",
      "Anne: My office, just hurry up, I gotta go see my client soon\n",
      "Kitty: On its way! 5 min\n",
      "Summary: Kitty forgot the keys and can't get inside. Anne offers to send the keys to Kitty via Uber. Kitty orders the Uber.\n",
      "Coherence score: 4\n",
      "Coherence rational: The summary effectively captures the main events of the conversation between Kitty and Anne, presenting them in a clear and logical order. It outlines Kitty's problem of forgetting the keys, Anne's offer to send them via Uber, and Kitty's action of ordering the Uber. However, it could be slightly improved by including more context about the urgency of the situation, which would enhance the overall coherence and depth of the summary.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'''\n",
    "Example Record:\n",
    "Dialogue: \t{output_df[\"dialogue\"][0]}\n",
    "Summary: {output_df[\"summary\"][0]}\n",
    "Coherence score: {output_df[\"coherence\"][0]}\n",
    "Coherence rational: {output_df[\"coherence_rationale\"][0]}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, as a way to measure overall error, we are subtracting each row's individual score from the max score it can receieve for that category. Then we will add up all of the error together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error Scores:\n",
      "coherence: 356\n",
      "consistency: 445\n",
      "fluency: 300\n",
      "relevance: 407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_df[\"coherence_error\"] = 5 - output_df[\"coherence\"]\n",
    "output_df[\"consistency_error\"] = 5 - output_df[\"consistency\"]\n",
    "output_df[\"fluency_error\"] = 3 - output_df[\"fluency\"]\n",
    "output_df[\"relevance_error\"] = 5 - output_df[\"relevance\"]\n",
    "\n",
    "print(f'''\n",
    "Error Scores:\n",
    "coherence: {output_df[\"coherence\"].sum()}\n",
    "consistency: {output_df[\"consistency\"].sum()}\n",
    "fluency: {output_df[\"fluency\"].sum()}\n",
    "relevance: {output_df[\"relevance\"].sum()}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another measure, let's look at the average score for each category.  \n",
    "\n",
    "As you can see, fluency got a perfect score since it was pertaining to the grammar of the summaries. However, coherency, which focuses on collective quality, was ranked the lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Error Scores:\n",
      "coherence: 3.56\n",
      "consistency: 4.45\n",
      "fluency: 3.0\n",
      "relevance: 4.07\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'''\n",
    "Average Error Scores:\n",
    "coherence: {output_df[\"coherence\"].mean()}\n",
    "consistency: {output_df[\"consistency\"].mean()}\n",
    "fluency: {output_df[\"fluency\"].mean()}\n",
    "relevance: {output_df[\"relevance\"].mean()}\n",
    "''')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
